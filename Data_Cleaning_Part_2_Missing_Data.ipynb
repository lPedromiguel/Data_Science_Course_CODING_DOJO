{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lPedromiguel/Data_Science_Course_CODING_DOJO/blob/main/Data_Cleaning_Part_2_Missing_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW4aKXeafcra"
      },
      "source": [
        "# Data Cleaning: Missing Values\n",
        "\n",
        "\n",
        "\n",
        "> \"Garbage in, garbage out.\"  \n",
        "\n",
        "Let's clean up the garbage!\n",
        "\n",
        "In this notebook we will:\n",
        "\n",
        "1. Review dropping unnecessary columns and duplicates, correcting datatypes, and fixing poorly formed categories\n",
        "2. Identify columns with missing values\n",
        "3. Choose and implement strategies for dealing with missing values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPmQXuKXeIfb"
      },
      "source": [
        "# Import Pandas\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Data\n",
        "\n",
        "We will be using a slightly altered version of the Titanic dataset, which is a common practice dataset for learning data science.\n",
        "\n",
        "The dataset contains information about passengers on The Titanic as well a column describing whether they survived the accident or not.  The ultimate goal of most predictions on this dataset is to predict who will survive based on the other information.  Our goal will be to clean the data and make it ready for analysis.\n",
        "\n",
        "Each row contains data about one passenger.\n",
        "\n",
        "# Data Dictionary\n",
        "\n",
        "Sometimes your data will come with a dictionary that helps you understand what the columns mean and what data types should be in them.  This is useful for cleaning the data.\n",
        "\n",
        "| Column   | Description                                | Value Keys                                     | Type  |\n",
        "|----------|--------------------------------------------|------------------------------------------------|-------|\n",
        "| survival | Survival                                   | 0 = No, 1 = Yes                                | int   |\n",
        "| pclass   | Ticket class                               | 1 = 1st, 2 = 2nd, 3 = 3rd                      | int   |\n",
        "| sex      | Sex                                        |                                                | string   |\n",
        "| Age      | Age in years                               |                                                | int   |\n",
        "| sibsp    | # of siblings / spouses aboard the Titanic |                                                | int   |\n",
        "| parch    | # of parents / children aboard the Titanic |                                                | int   |\n",
        "| ticket   | Ticket number                              |                                                | string   |\n",
        "| fare     | Passenger fare                             |                                                | float |\n",
        "| cabin    | Cabin number                               |                                                | string   |\n",
        "| embarked | Port of Embarkation                        | C = Cherbourg, Q = Queenstown, S = Southampton | string   |\n"
      ],
      "metadata": {
        "id": "6MZ1gvv3CQ_F"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1VDPfCpfgd8"
      },
      "source": [
        "# Load the Data\n",
        "df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vQ7Xta6T-hqBFfQTUMNSVdl03z74KlyecepwOK0Y4s_zv0CG6L8NrVP8tlBsnV2tI4W_cHE5NB9cduU/pub?output=csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## df.head()\n",
        "`df.head()` is one of our most important methods.  It lets us examine the first 5 rows of our dataframe and as many columns as our maximum display columns configuration will allow.\n",
        "\n",
        "Look below and try to decide which columns will be useful for analysis and predictions, and which columns probably do not any information that would be relevant to who might survive the catastrophic sinking of The Titanic."
      ],
      "metadata": {
        "id": "W4-wcqVVBqeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the first 5 rows\n",
        "df.head()"
      ],
      "metadata": {
        "id": "-2r_T6YzB3pW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove Duplicates\n",
        "\n",
        "We don't want any duplicate entries in our data.  This will skew our analysis and confuse our predictive models.  Duplicate entries are quite common in data that has not been cleaned.\n",
        "\n",
        "We can use `df.duplicated()` to show whether rows are duplicates."
      ],
      "metadata": {
        "id": "wu3lVBeRNrvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicates\n",
        "df.duplicated()"
      ],
      "metadata": {
        "id": "bgc42ditN6nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, we don't want to have to look row by row, so let's use `.sum()` to add up all of the `True` values.  When using `.sum()`, a `True` value will evaluate to a 1."
      ],
      "metadata": {
        "id": "vxhSttCON_Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the duplicates\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "DkhFqT2rN-c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 122 duplicated rows.  We can simply remove those with df.drop_duplicates()."
      ],
      "metadata": {
        "id": "NzkKuX07ON1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicates\n",
        "df = df.drop_duplicates()\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "aCnIcZsxOVPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove Unnecessary Columns\n",
        "\n",
        "The `Unnamed: 0` column is probably a duplicate of the index.  You'll see this sometimes as Pandas tries to assign a new index when one already exists in the data.  We can drop that.\n",
        "\n",
        "The `PassengerID` is a numbering of passengers.  Most likely the passenger number will not affect whether they passenger survided.  They also all have separate values, but are not ordered in any way and do not describe any quantitative feature of the passenger.  We will remove that.\n",
        "\n",
        "The `Name` column is the passenger's name.  There is probably little or no correlation between a passenger's name and whether they survived.  Like `PassengerID` they are probably unique to each row and probably won't be useful for analysis or prediction.  We will remove that."
      ],
      "metadata": {
        "id": "-XU9FaI9C6_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove unnecessary columns\n",
        "df = df.drop(columns=['Unnamed: 0', 'PassengerId', 'Name'])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Cgxz0PoxFCEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further study of columns with df.nunique()\n",
        "\n",
        "`.nunique()` will tell us how many unique values are in each column.  This can give us a clue about the liklihood of finding a relationship between the column value and other data."
      ],
      "metadata": {
        "id": "pOb6U9o8FPEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the number of unique values in columns\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "u9UHT19xFob6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Ticket` is a category, but there are almost as many different unique values as there are rows in the dataframe.  If the Ticket data is different for most every passenger and it's not a number describing some quality of the passenger, like 'Age', then it probably won't be very useful.  We will drop that, too.\n",
        "\n",
        "## Notice\n",
        "We are using judgement calls here.  Data scientists often must use their judgement, and it's often useful to consult a subject matter expert to double check our judgement when possible."
      ],
      "metadata": {
        "id": "HuB76OI0G1Ku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the Ticket column\n",
        "df = df.drop(columns='Ticket')"
      ],
      "metadata": {
        "id": "d7cWwO0fH_LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correct Data Types"
      ],
      "metadata": {
        "id": "yCahCyxPOvwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## .info() \n",
        "`df.info()` is a very useful method.  It tells us a lot about our data.  What information can you gain from looking at the output below?"
      ],
      "metadata": {
        "id": "hvvCMwP3BTe-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pFUY4lUfh2X"
      },
      "source": [
        "# Check columns, rows, data types, and missing values\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that there are 906 entries (rows), 9 columns remaining, and we can see how many values there are in each column and what their datatype is.\n",
        "\n",
        "`Age` has a few missing values, `Cabin` has a lot of missing values, and `Embarked has just 2 missing values.  We will learn how to deal with those in another lesson.\n",
        "\n",
        "Also notice that `SibSp` is an object data type.  That column represents the number of siblings and spouses a passenger has aboard with them, so it should be an integer.  Let's examine that more closely."
      ],
      "metadata": {
        "id": "J6fOscPcIEHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correcting Datatypes\n",
        "\n",
        "We can use `df[column].astype(type)` to change the datatype of a column. For instance we want to change `df['SibSp']` from an object to an integer."
      ],
      "metadata": {
        "id": "TSK1_JV8MaXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the SibSp column from an object to an integer\n",
        "df['SibSp'] = df['SibSp'].astype(int)"
      ],
      "metadata": {
        "id": "lZAd1MYyMUqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Errors Changing Types\n",
        "\n",
        "Oh!  We got an error!  If we look carefully at the bottom of the error it says: `ValueError: invalid literal for int() with base 10: 'one'`\n",
        "\n",
        "It says it can't change the string 'one' to an integer.  That means one of the values is 'one' instead of 1!\n",
        "\n",
        "We can use `.replace` to change those, even if there are more than one of them."
      ],
      "metadata": {
        "id": "8fFBwUYyMqmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace the bad value in SibSp and try again to change the type\n",
        "df['SibSp'] = df['SibSp'].replace('one', 1)\n",
        "df['SibSp'] = df['SibSp'].astype(int)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "v-TP9Bh0NDle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`df['Age']` should also be an integer, but it is shown as a float.  This usually doesn't matter very much, as machine learning and most visualizations will display ints and floats the same.  But, what happens if we try to change the datatype?"
      ],
      "metadata": {
        "id": "UjKb5Om5gxDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Age'] = df['Age'].astype(int)"
      ],
      "metadata": {
        "id": "G2WgfdDThBVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `df['Age']` contains NA values, or missing values.  Missing values cannot be converted to integer columns.  When you see a column that should be an integer that is a float, often times this is the reason.\n",
        "\n",
        "We will deal with the missing values in the 'Age' column in another lesson, so we will leave it as float type data for now."
      ],
      "metadata": {
        "id": "DNEYp1KphHcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check for Inconsistencies in Categorical Values"
      ],
      "metadata": {
        "id": "g01_riGkPhOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another common problem with dirty data will be categories that are not uniform.  This can include misspellings, abbreviations or typos.\n",
        "\n",
        "We can use `.value_counts()` again to examine our `object` type columns.  If we do this with numeric columns our outputs can often be too long to read easily.  It works best with categorical variables or columns."
      ],
      "metadata": {
        "id": "YiDR8q8sPmFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the unique values and counts of the 'Sex' column\n",
        "df['Sex'].value_counts()"
      ],
      "metadata": {
        "id": "f1UsTAOaAst0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that there are both 'female' and 'F' values.  We can assume these are meant to be the same, so we will replace 'F' with 'female' so that our categories are uniform."
      ],
      "metadata": {
        "id": "qDgkTmIvQLTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correct Incorrect Categories"
      ],
      "metadata": {
        "id": "mLdr1TqvRm2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the values in the Sex column\n",
        "df['Sex'] = df['Sex'].replace('F', 'female')\n",
        "df['Sex'].value_counts()"
      ],
      "metadata": {
        "id": "ru9_wxwqQJVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identify Missing Values"
      ],
      "metadata": {
        "id": "9gSsaAQRQyl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "QKiXdVHhNahS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'Age', 'Cabin', and 'Embarked' are all missing values."
      ],
      "metadata": {
        "id": "XrBiuLFMNisJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dropping Rows\n",
        "\n",
        "Only 2 rows are missing values in the 'Embarked' column.  We will just drop those 2 rows since they represent a very small percentage of our data."
      ],
      "metadata": {
        "id": "SKZmL35TNmyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(subset=['Embarked'], inplace=True)\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "MMIqBn_mNzt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dropping Columns\n",
        "\n",
        "The 'Cabin' column is missing a significant amount of data."
      ],
      "metadata": {
        "id": "wHSz0yJLN5hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_missing = df['Cabin'].isna().sum()\n",
        "\n",
        "total_rows = df.shape[0]\n",
        "\n",
        "percent_missing = num_missing / total_rows\n",
        "print(f'{percent_missing:.2f}% of the data in the Cabin column is missing')"
      ],
      "metadata": {
        "id": "pBNlcKqwOKL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since it contains less than 50% non-null values, we will just drop that whole column.  We won't be able to analyze if everyone in the same cabin either survived or not, but we could always bring it back if we really needed to do that with the 23% of rows that have that information."
      ],
      "metadata": {
        "id": "oCavdtnHQKej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns='Cabin', inplace=True)\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "i66J1L4FNpBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Too many rows are missing data in the 'Age' column to drop the rows, but not enough to drop the column.  Instead we will impute the missing values by filling them with the median value of the ages of all of the passengers.  This will introduce a lot of errors into our dataframe, but hopefully imputing with an average age will minimize those errors.\n",
        "\n",
        "We might be able to do more complex imputation by looking carefully at each row for clues to how old each passenger might be, but we aren't going to do that in this example."
      ],
      "metadata": {
        "id": "Az84VgVBQZMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "age_median = df['Age'].median()\n",
        "df['Age'].fillna(age_median, inplace=True)\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "pWO8Xw2iQ2dS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now also correct the datatype for `df['Age']`"
      ],
      "metadata": {
        "id": "-ZVdaHlTh3lW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Age'] = df['Age'].astype(int)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "ugj5cNQih-Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NOTE:\n",
        "The above imputation strategy is ONLY appropriate for preparing data for analysis.  You would not use this imputation technique when preparing data for machine learning.  You'll learn why in the next stack.\n",
        "\n",
        "We have eliminated the missing data from the dataframe and the dataset is now ready for analysis and machine learning."
      ],
      "metadata": {
        "id": "8dPL9J8vRCch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "In this lesson you reviewed data cleaning steps and learned how identify missing data, choose a strategy for dealing with it, and how to drop columns, drop rows, or use a simple imputation strategy to fill in missing data while minimizing the error you introduce into your data."
      ],
      "metadata": {
        "id": "t6j71xo9Q-KF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dAjCdfDUilEt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}